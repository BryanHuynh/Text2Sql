{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba4f6b6-7eb0-43e3-a33d-ec3ef8c90d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "445d27ca-5bee-4f6a-963d-ac7357b9661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    with open(filepath) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "train_data = load_data(\"./spider_data/train_spider.json\")\n",
    "train_tables_data = load_data(\"./spider_data/tables.json\")\n",
    "\n",
    "test_data = load_data(\"./spider_data/test.json\")\n",
    "test_tables_data = load_data(\"./spider_data/test_tables.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "201a75eb-adce-45d3-b1c5-556c154e160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_index(tables):\n",
    "    index_dictionary = {}\n",
    "    for index, table in enumerate(tables):\n",
    "        index_dictionary[table['db_id']] = index\n",
    "    return index_dictionary\n",
    "\n",
    "train_table_index_dictionary = generate_table_index(train_tables_data)\n",
    "test_table_index_dictionary = generate_table_index(test_tables_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f8c261-7fbe-4970-af77-3216b029822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_table(db_id, tables, table_index_dictionary):\n",
    "    table = tables[table_index_dictionary[db_id]]\n",
    "    table_names = []\n",
    "    column = {}\n",
    "    # print(table['column_names_original'])\n",
    "    for table_name in table['table_names_original']:\n",
    "        table_names.append(table_name)\n",
    "        column[table_name] = []\n",
    "    for index, column_name in table['column_names_original']:\n",
    "        if(index == -1): \n",
    "            continue\n",
    "        column[table_names[index]].append(column_name)\n",
    "    return column\n",
    "\n",
    "def parse(entry, tables, table_index_dictionary):\n",
    "    question = entry['question']\n",
    "    target = entry['query']\n",
    "    db_id = entry['db_id']\n",
    "    schema = parse_table(db_id, tables, table_index_dictionary)\n",
    "    # print(schema, question, target)\n",
    "    return {'input': f\"Given the following SQL Schema: {schema}. Provide a SQL query reponse for: {question}\", 'target': target}\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = [item for item in examples['input']]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    targets = [item for item in examples['target']]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=521, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78dcb5aa-931b-45e1-907a-17d1a0ff7fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"Given the following SQL Schema: {'department': ['Department_ID', \"\n",
      "          \"'Name', 'Creation', 'Ranking', 'Budget_in_Billions', \"\n",
      "          \"'Num_Employees'], 'head': ['head_ID', 'name', 'born_state', 'age'], \"\n",
      "          \"'management': ['department_ID', 'head_ID', 'temporary_acting']}. \"\n",
      "          'Provide a SQL query reponse for: How many heads of the departments '\n",
      "          'are older than 56 ?',\n",
      " 'target': 'SELECT count(*) FROM head WHERE age  >  56'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702eed9c5fdc42e1930007cea261fa85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan/Projects/Text2Sql/cuda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae129858d43246e0a662a885508d51f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 7000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2147\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_training_data = [parse(train_data[i], train_tables_data, train_table_index_dictionary) for i in range(len(train_data))]\n",
    "pprint(raw_training_data[0])\n",
    "raw_testing_data = [parse(test_data[i], test_tables_data, test_table_index_dictionary) for i in range(len(test_data))]\n",
    "training_dataset = Dataset.from_list(raw_training_data)\n",
    "testing_dataset = Dataset.from_list(raw_testing_data)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": training_dataset,\n",
    "    \"test\": testing_dataset\n",
    "})\n",
    "encoded_dataset = dataset_dict.map(preprocess_data, batched=True)\n",
    "encoded_dataset = encoded_dataset.remove_columns(['input', 'target'])\n",
    "pprint(encoded_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe42d5e0-792b-41ee-bb80-46069bd138ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query(db_id, query, Test=False):\n",
    "    if Test == False:\n",
    "        db_directory = './spider_data/database'\n",
    "    else:\n",
    "        db_directory = './spider_data/test_database'\n",
    "    con = sqlite3.connect(f\"{db_directory}/{db_id}/{db_id}.sqlite\")\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        res = cur.fetchall()\n",
    "    except:\n",
    "        res = -1\n",
    "    finally:\n",
    "        con.close()\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62bbb1e-25f1-4603-beb5-54be81c39dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Separate metadata from model inputs\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    # metadata = [item[\"metadata\"] for item in batch]  # Collect metadata separately\n",
    "\n",
    "    # Convert to tensors for model input\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels),\n",
    "        # \"metadata\": metadata,  # Keep metadata for later use\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c500519a-ee8f-427f-9d30-1179929c8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "model.to(device)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "train_loader = DataLoader(encoded_dataset['train'], batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "eval_loader = DataLoader(encoded_dataset['test'], batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172136b-1559-4115-947f-389904177224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='266' max='8750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 266/8750 00:42 < 22:54, 6.17 it/s, Epoch 0.30/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    remove_unused_columns=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['test'],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator\n",
    "    # compute_loss_func=custom_loss\n",
    ")\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca9c4f-5a83-4331-9b6f-b411ef32461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./fine_tuned_t5_sql\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_t5_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2af448-989f-44bc-a435-97e25b094eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
